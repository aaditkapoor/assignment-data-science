{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We aim to find the keywords in file.pdf and create a distribution chart of the same.\n",
    "### Using File: file.pdf\n",
    "### Stopwords file: stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2 # For extracting text\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize # For spliting\n",
    "from string import punctuation\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the stopwords from file\n",
    "def load_stopwords():\n",
    "    fp = open(\"stopwords.txt\",\"r\")\n",
    "    words = []\n",
    "    for word in fp.readlines():\n",
    "        words.append(word.replace(\"\\n\",\"\")) # Getting words\n",
    "    return words # filtered words\n",
    "\n",
    "# Extracts text from the pdf\n",
    "def extract_text_from_pdf():\n",
    "    data = {}\n",
    "    pdfFileObject = open('file.pdf', 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    "    count = pdfReader.numPages # Number of pages in the pdf\n",
    "    print (\"Number of pages in pdf: \", count)\n",
    "    \n",
    "    for i in range(count):\n",
    "        data[\"page\" + str(i)] = [] # Building empty data store variable in the form data = {'page0':[], 'page1':[]}\n",
    "        \n",
    "    for i in range(count):\n",
    "        page = pdfReader.getPage(i) # Getting each page\n",
    "        data[\"page\"+str(i)].append(page.extractText().lower()) # Getting text from each page\n",
    "    \n",
    "    return data # Getting the extracted text\n",
    "\n",
    "\n",
    "stopwords = load_stopwords() # Loading stopwords\n",
    "\n",
    "# Removes stopwords and punctuation\n",
    "def remove_stopwords_punc(sentences):\n",
    "    global stopwords\n",
    "    new = []\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in stopwords and word not in punctuation:\n",
    "                new.append(word) # Removing all the stopwords and assigning it into a list\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in pdf:  23\n"
     ]
    }
   ],
   "source": [
    "texts = extract_text_from_pdf() # Extracting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of tokenized_words:  23\n"
     ]
    }
   ],
   "source": [
    "# We now have a list of lists\n",
    "tokenized_words = [sentence[0].split() for sentence in texts.values()] # Splitting words into list\n",
    "print (\"length of tokenized_words: \", len(tokenized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of filtered words:  2446\n"
     ]
    }
   ],
   "source": [
    "filtered_words = remove_stopwords_punc(tokenized_words) # All the filtered words\n",
    "print (\"length of filtered words: \", len(filtered_words)) # length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will find the most common words from these tokens\n",
    "words_count = {}\n",
    "for word in filtered_words:\n",
    "    words_count[word] = 0 # Intializing all the words to 0 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting common words count\n",
    "repeating_words = list(set(filtered_words))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the common words\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(filtered_words) # Creating a counter\n",
    "print (\"Getting the common words\")\n",
    "common_words = dict(counter.most_common())\n",
    "vals = list(common_words.values())\n",
    "\n",
    "def remove_ones(vals, mean=None):\n",
    "    vals[:] = [val for val in vals if val != 1] # Removing all ones from list\n",
    "    return vals\n",
    "\n",
    "                \n",
    "vals = remove_ones(vals) # Removing ones from common_words numbers\n",
    "npvalues = np.array(vals) # Changing into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 82\n",
      "Min:  2\n",
      "Mean:  4.848275862068966\n"
     ]
    }
   ],
   "source": [
    "max_occurence_count = np.max(npvalues) # max count\n",
    "min_occurence_count = np.min(npvalues) # min\n",
    "mean_occurence_count = np.mean(npvalues) # Mean value of occurences\n",
    "\n",
    "\n",
    "print (\"Max:\", max_occurence_count)\n",
    "print (\"Min: \", min_occurence_count)\n",
    "print (\"Mean: \", mean_occurence_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the final list of keywords and the max,min range of words\n",
    "def get_max_min_word(common_words):\n",
    "    max_word = \"\"\n",
    "    min_word = \"\"\n",
    "    keywords = [] # All the max. words\n",
    "    for i in common_words:\n",
    "        if common_words[i] == max_occurence_count:\n",
    "            max_word = i\n",
    "            keywords.append(max_word)\n",
    "        if common_words[i] == min_occurence_count:\n",
    "            min_word = i\n",
    "   \n",
    "    print (\"Max word is: \", max_word)\n",
    "    print (\"Min word is: \", min_word)\n",
    "    return keywords\n",
    "\n",
    "# Calculates score of each possible keyword\n",
    "def calculate_percentage(common_words):\n",
    "    percentages = {}\n",
    "    for k in common_words:\n",
    "        if common_words[k] != 1: # Discarding all the '1' value common words\n",
    "            percentages[k] = common_words[k] / len(vals) * 100 # Calculating percentage of the possible keywords\n",
    "    return percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word is:  java\n",
      "Min word is:  time\n"
     ]
    }
   ],
   "source": [
    "keywords = get_max_min_word(common_words) # the important keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>java</td>\n",
       "      <td>28.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new</td>\n",
       "      <td>14.827586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>//</td>\n",
       "      <td>12.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-2003</td>\n",
       "      <td>7.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jguru.com.</td>\n",
       "      <td>7.931034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word      score\n",
       "0        java  28.275862\n",
       "1         new  14.827586\n",
       "2          //  12.758621\n",
       "3   1996-2003   7.931034\n",
       "4  jguru.com.   7.931034"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentages = calculate_percentage(common_words) # scores\n",
    "csv_data = pd.DataFrame(list(percentages.items()), columns=['word','score']) # Changing into pandas datafram\n",
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data.to_csv(\"keyword_score.csv\") # Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
